<!doctype html>
<html lang="es">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Práctica: Fine-tunning (Capturas)</title>
  <style>
    :root { --max: 980px; --bg:#0b0f17; --card:#121a2a; --text:#e8eefc; --muted:#b9c3da; --line:#26324a; }
    body { margin:0; font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; background:var(--bg); color:var(--text); }
    header, main, footer { max-width: var(--max); margin: 0 auto; padding: 24px; }
    header h1 { margin: 0 0 8px; font-size: 1.6rem; }
    header p { margin: 0; color: var(--muted); line-height: 1.45; }
    .card { background: var(--card); border: 1px solid var(--line); border-radius: 12px; padding: 16px; margin: 16px 0; }
    figure { margin: 0; }
    img { width: 100%; height: auto; border-radius: 10px; display:block; border: 1px solid var(--line); }
    figcaption { margin-top: 10px; color: var(--muted); font-size: 0.95rem; line-height: 1.35; }
    footer { color: var(--muted); font-size: 0.9rem; }
    code { background: rgba(255,255,255,0.06); padding: 2px 6px; border-radius: 6px; }
    h2 { margin: 0 0 10px; font-size:1.15rem; }
  </style>
</head>

<body>
  <header>
    <h1>Práctica: Fine-tunning</h1>
    <p>
      En esta práctica preparo un fine-tunning con LoRA sobre un modelo Gemma usando herramientas de MLX/MLX-LM, creando un dataset en
      <code>.jsonl</code>, entrenando el adaptador y validando el resultado con una prueba de generación. Un flujo típico con MLX-LM usa
      <code>mlx_lm.lora</code> indicando modelo, datos y ruta del adaptador. [web:101][web:99]
    </p>
  </header>

  <main>
    <!-- 1 -->
    <section class="card" id="captura-1">
      <figure>
        <img src="img/1.png" alt="Captura 1: preparación del entorno e instalación de dependencias" loading="lazy">
        <figcaption>
          <strong>Captura 1 — Preparación del entorno.</strong>
          Se crea el directorio del proyecto, un entorno virtual (<code>venv</code>), se actualiza <code>pip</code> y se instalan dependencias
          (por ejemplo <code>mlx-lm</code>, <code>datasets</code>, <code>huggingface_hub</code>, etc.) para poder hacer el fine-tuning en local.
        </figcaption>
      </figure>
    </section>

    <!-- 2 (CORREGIDA) -->
    <section class="card" id="captura-2">
      <figure>
        <img src="img/2.png" alt="Captura 2: inicio de sesión en Hugging Face CLI con token" loading="lazy">
        <figcaption>
          <strong>Captura 2 — Inicio de sesión en Hugging Face.</strong>
          Aquí inicio sesión en la CLI de Hugging Face para autenticarme con un token y poder descargar modelos/datasets (y, si hiciera falta,
          subir artefactos). El propio aviso indica que el login clásico está siendo reemplazado por el comando nuevo <code>hf auth login</code>. [web:114][web:107]
        </figcaption>
      </figure>
    </section>

    <!-- 3 -->
    <section class="card" id="captura-3">
      <figure>
        <img src="img/3.png" alt="Captura 3: creación de train.jsonl y valid.jsonl y verificación de líneas" loading="lazy">
        <figcaption>
          <strong>Captura 3 — Dataset (train/valid) y verificación.</strong>
          Se crean los ficheros <code>train.jsonl</code> y <code>valid.jsonl</code> en <code>data/</code> con ejemplos tipo “messages”.
          También se comprueba el número de líneas y se corrigen errores típicos al escribir JSON a mano antes de entrenar.
        </figcaption>
      </figure>
    </section>

    <!-- 4 -->
    <section class="card" id="captura-4">
      <figure>
        <img src="img/4.png" alt="Captura 4: entrenamiento LoRA con mlx_lm.lora mostrando métricas y guardado del adaptador" loading="lazy">
        <figcaption>
          <strong>Captura 4 — Entrenamiento LoRA.</strong>
          Se ejecuta el entrenamiento con <code>mlx_lm.lora</code> indicando el modelo base, la carpeta de datos, las iteraciones y la ruta del
          adaptador (<code>--adapter-path</code>). Se ven métricas como <em>train loss</em> y <em>val loss</em>, y al final se guardan los pesos
          del adaptador (por ejemplo <code>adapters.safetensors</code>). [web:99][web:101]
        </figcaption>
      </figure>
    </section>

    <!-- 5 -->
    <section class="card" id="captura-5">
      <figure>
        <img src="img/5.png" alt="Captura 5: prueba de resultado del modelo afinado" loading="lazy">
        <figcaption>
          <strong>Captura 5 — Prueba del modelo afinado.</strong>
          Se realiza una prueba de generación para validar que el modelo, junto con el adaptador LoRA, responde con el estilo/tema del dataset.
          Esta parte es donde se comprueba si el fine-tunning ha merecido la pena más allá de las métricas. [web:99]
        </figcaption>
      </figure>
    </section>

    <!-- 6 -->
    <section class="card" id="captura-6">
      <figure>
        <img src="img/6.png" alt="Captura 6: modelo utilizado para el fine-tuning (Gemma 3 270M en MLX, variantes)" loading="lazy">
        <figcaption>
          <strong>Captura 6 — Modelo base utilizado.</strong>
          Se muestra el modelo elegido como base para el fine-tunning (Gemma 3 270M en variantes para MLX, por ejemplo bf16/8bit/4bit), dejando
          claro desde qué checkpoint se parte antes de aplicar LoRA. [web:99]
        </figcaption>
      </figure>
    </section>

    <section class="card" id="conclusion">
      <h2>Conclusión</h2>
      <p style="margin:0; color: var(--muted); line-height: 1.45;">
        Con esta práctica me quedo con que el fine-tunning no es solo “entrenar y ya”: primero tienes que dejar el entorno y el acceso al Hub bien
        montados, luego preparar un dataset en formato correcto, entrenar el adaptador y, por último, probar con preguntas reales para ver si de
        verdad ha cambiado el comportamiento del modelo.
      </p>
    </section>
  </main>

  <footer>
    <p style="margin:0;">Práctica documentada con capturas.</p>
  </footer>
</body>
</html>
